{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0e9f51-dae7-49ed-965a-f5aa40c7da1b",
   "metadata": {},
   "source": [
    "# Neo4j 环境设置\n",
    "使用本地环境，同时安装 apoc 插件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274ab3d2-30dc-4862-8afb-52cbb06e0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "566dae6b-e857-43e1-ad43-e57d755ddebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| graph: <langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7fc84ba4dac0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.graphs.neo4j_graph.Neo4jGraph at 0x7fc84ba4dac0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "url = \"neo4j://localhost:7687\"\n",
    "username =\"neo4j\"\n",
    "password = \"langchain\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=url, \n",
    "    username=username, \n",
    "    password=password\n",
    ")\n",
    "ic(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279a236-7082-456e-b220-73403b0347ef",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "使用一个综合数据集。该数据集是 ChatGPT 辅助的。这是一个只有 100 个节点的小型数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f86c282-696d-4b91-8837-1871a96ecb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://gist.githubusercontent.com/tomasonjo/08dc8ba0e19d592c4c3cde40dd6abcc3/raw/da8882249af3e819a80debf3160ebbb3513ee962/microservices.json\"\n",
    "import_query = requests.get(url).json()['query']\n",
    "graph.query(\n",
    "    import_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aefaaa-42a6-49a4-b14a-44968cc79c83",
   "metadata": {},
   "source": [
    "## Neo4j 向量索引\n",
    "首先实现向量索引搜索，通过名称和描述查找相关任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b27529cf-81ef-41ce-9555-d1aa2b3f6082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| vector_index: <langchain_community.vectorstores.neo4j_vector.Neo4jVector object at 0x7fc84df2c220>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.neo4j_vector.Neo4jVector at 0x7fc84df2c220>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-x\"\n",
    "url = \"neo4j://localhost:7687\"\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    index_name='tasks',\n",
    "    node_label=\"Task\",\n",
    "    text_node_properties=['name', 'description', 'status'],\n",
    "    embedding_node_property='embedding',\n",
    ")\n",
    "ic(vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c02bb4d4-2a85-4450-9b40-5c7d01e4c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| response[0].page_content: ('\n",
      "                              '\n",
      "                               'name: RecommendationFeature\n",
      "                              '\n",
      "                               'description: Add feature to RecommendationService\n",
      "                              '\n",
      "                               'status: In Progress')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nname: RecommendationFeature\\ndescription: Add feature to RecommendationService\\nstatus: In Progress'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vector_index.similarity_search(\n",
    "    \"How will RecommendationService be updated?\"\n",
    ")\n",
    "ic(response[0].page_content)\n",
    "# name: BugFix\n",
    "# description: Add a new feature to RecommendationService to provide ...\n",
    "# status: In Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1409e28-44d1-4ac8-b2b6-9b1e8e859a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How will recommendation service be updated?',\n",
       " 'result': \"I don't have enough information to provide a specific answer on how the recommendation service will be updated. If there are any specific tasks or projects related to updating the recommendation service, please provide more details so I can assist you better.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_index.as_retriever()\n",
    ")\n",
    "vector_qa.invoke(\n",
    "    \"How will recommendation service be updated?\"\n",
    ")\n",
    "# The RecommendationService is currently being updated to include a new feature \n",
    "# that will provide more personalized and accurate product recommendations to \n",
    "# users. This update involves leveraging user behavior and preference data to \n",
    "# enhance the recommendation algorithm. The status of this update is currently\n",
    "# in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e644a38-3e28-45ee-a689-39e3bb2cf420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How many open tickets are there?',\n",
       " 'result': 'There are 3 open tickets currently.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_qa.invoke(\n",
    "    \"How many open tickets are there?\"\n",
    ")\n",
    "# There are 4 open tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85714ccd-4cdd-4e2b-8ae6-097d63c0d3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count(*)': 4}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\n",
    "    \"MATCH (t:Task {status:'Open'}) RETURN count(*)\"\n",
    ")\n",
    "# [{'count(*)': 5}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f013cb32-aa07-455b-8c78-4d485f1b7a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| cypher_chain: GraphCypherQAChain(verbose=True, graph=<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7fc84ba4dac0>, cypher_generation_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'schema'], template='Task:Generate Cypher statement to query a graph database.\n",
      "                  Instructions:\n",
      "                  Use only the provided relationship types and properties in the schema.\n",
      "                  Do not use any other relationship types or properties that are not provided.\n",
      "                  Schema:\n",
      "                  {schema}\n",
      "                  Note: Do not include any explanations or apologies in your responses.\n",
      "                  Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
      "                  Do not include any text except the generated Cypher statement.\n",
      "                  \n",
      "                  The question is:\n",
      "                  {question}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc8346536d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83465c460>, model_name='gpt-4', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), qa_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant that helps to form nice and human understandable answers.\n",
      "                  The information part contains the provided information that you must use to construct an answer.\n",
      "                  The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
      "                  Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
      "                  Here is an example:\n",
      "                  \n",
      "                  Question: Which managers own Neo4j stocks?\n",
      "                  Context:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\n",
      "                  Helpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\n",
      "                  \n",
      "                  Follow this example when generating answers.\n",
      "                  If the provided information is empty, say that you don't know the answer.\n",
      "                  Information:\n",
      "                  {context}\n",
      "                  \n",
      "                  Question: {question}\n",
      "                  Helpful Answer:\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834661f10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83466a880>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), graph_schema='Node properties are the following:\n",
      "                  Chunk {id: STRING, source: STRING, text: STRING, page: INTEGER, textEmbedding: LIST},PDF {name: STRING},Microservice {name: STRING, technology: STRING},Dependency {name: STRING, type: STRING},Task {description: STRING, name: STRING, status: STRING, embedding: LIST},Team {name: STRING},Person {name: STRING}\n",
      "                  Relationship properties are the following:\n",
      "                  \n",
      "                  The relationships are the following:\n",
      "                  (:Chunk)-[:PART_OF]->(:PDF),(:Microservice)-[:DEPENDS_ON]->(:Microservice),(:Microservice)-[:DEPENDS_ON]->(:Dependency),(:Microservice)-[:MAINTAINED_BY]->(:Team),(:Task)-[:ASSIGNED_TO]->(:Team),(:Task)-[:LINKED_TO]->(:Microservice),(:Person)-[:MEMBER_OF]->(:Team),(:Person)-[:LEAD_OF]->(:Team)')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphCypherQAChain(verbose=True, graph=<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7fc84ba4dac0>, cypher_generation_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'schema'], template='Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc8346536d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83465c460>, model_name='gpt-4', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), qa_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nHere is an example:\\n\\nQuestion: Which managers own Neo4j stocks?\\nContext:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\\nHelpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\\n\\nFollow this example when generating answers.\\nIf the provided information is empty, say that you don't know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834661f10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83466a880>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), graph_schema='Node properties are the following:\\nChunk {id: STRING, source: STRING, text: STRING, page: INTEGER, textEmbedding: LIST},PDF {name: STRING},Microservice {name: STRING, technology: STRING},Dependency {name: STRING, type: STRING},Task {description: STRING, name: STRING, status: STRING, embedding: LIST},Team {name: STRING},Person {name: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Chunk)-[:PART_OF]->(:PDF),(:Microservice)-[:DEPENDS_ON]->(:Microservice),(:Microservice)-[:DEPENDS_ON]->(:Dependency),(:Microservice)-[:MAINTAINED_BY]->(:Team),(:Task)-[:ASSIGNED_TO]->(:Team),(:Task)-[:LINKED_TO]->(:Microservice),(:Person)-[:MEMBER_OF]->(:Team),(:Person)-[:LEAD_OF]->(:Team)')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm = ChatOpenAI(temperature=0, model_name='gpt-4'),\n",
    "    qa_llm = ChatOpenAI(temperature=0), graph=graph, verbose=True,\n",
    ")\n",
    "ic(cypher_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6570fc01-9c9b-4c6e-beed-b52de34f63c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (t:Task {status: 'open'}) RETURN COUNT(t)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(t)': 0}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How many open tickets there are?',\n",
       " 'result': 'There are 0 open tickets.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.invoke(\n",
    "    \"How many open tickets there are?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae38c2b6-feaf-4bbc-b222-26a5dc7bf586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (t:Task)-[:ASSIGNED_TO]->(team:Team)\n",
      "WHERE t.status = 'open'\n",
      "RETURN team.name, COUNT(t) AS openTasks\n",
      "ORDER BY openTasks DESC\n",
      "LIMIT 1\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which team has the most open tasks?',\n",
       " 'result': \"I don't know the answer.\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.invoke(\n",
    "    \"Which team has the most open tasks?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a175dcc-6fd8-482a-872b-641a18902db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Microservice)-[:DEPENDS_ON]->(d:Dependency {type: 'Database'}) RETURN m.name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which services depend on Database directly?',\n",
       " 'result': \"I don't know the answer.\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.invoke(\n",
    "    \"Which services depend on Database directly?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9070f4b0-1925-4e42-a176-683ddd1950d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Microservice)-[:DEPENDS_ON*2..]->(d:Dependency {name: 'Database'}) RETURN m.name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'m.name': 'OrderService'}, {'m.name': 'OrderService'}, {'m.name': 'OrderService'}, {'m.name': 'OrderService'}, {'m.name': 'UserService'}, {'m.name': 'PaymentService'}, {'m.name': 'ShippingService'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which services depend on Database indirectly?',\n",
       " 'result': 'OrderService, UserService, PaymentService, ShippingService depend on Database indirectly.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.invoke(\n",
    "    \"Which services depend on Database indirectly?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7628816-d4a7-4e15-9d2f-63a7734c1b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuefeng/anaconda3/envs/langchain/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n",
      "ic| mrkl: AgentExecutor(verbose=True, tags=['openai-functions'], agent=OpenAIFunctionsAgent(llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc836eb01f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc836eb5b20>, model_name='gpt-4', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), tools=[Tool(name='Tasks', description='Useful when you need to answer questions about descriptions of tasks.\n",
      "                  Not useful for counting the number of tasks.\n",
      "                  Use full question as input.\n",
      "                  ', func=<bound method Chain.run of RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \n",
      "          If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "          ----------------\n",
      "          {context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834491190>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc834498fa0>, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Neo4jVector', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.neo4j_vector.Neo4jVector object at 0x7fc84df2c220>))>), Tool(name='Graph', description='Useful when you need to answer questions about microservices,\n",
      "                  their dependencies or assigned people. Also useful for any sort of \n",
      "                  aggregation like counting the number of tasks, etc.\n",
      "                  Use full question as input.\n",
      "                  ', func=<bound method Chain.run of GraphCypherQAChain(verbose=True, graph=<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7fc84ba4dac0>, cypher_generation_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'schema'], template='Task:Generate Cypher statement to query a graph database.\n",
      "          Instructions:\n",
      "          Use only the provided relationship types and properties in the schema.\n",
      "          Do not use any other relationship types or properties that are not provided.\n",
      "          Schema:\n",
      "          {schema}\n",
      "          Note: Do not include any explanations or apologies in your responses.\n",
      "          Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
      "          Do not include any text except the generated Cypher statement.\n",
      "          \n",
      "          The question is:\n",
      "          {question}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc8346536d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83465c460>, model_name='gpt-4', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), qa_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant that helps to form nice and human understandable answers.\n",
      "          The information part contains the provided information that you must use to construct an answer.\n",
      "          The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
      "          Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
      "          Here is an example:\n",
      "          \n",
      "          Question: Which managers own Neo4j stocks?\n",
      "          Context:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\n",
      "          Helpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\n",
      "          \n",
      "          Follow this example when generating answers.\n",
      "          If the provided information is empty, say that you don't know the answer.\n",
      "          Information:\n",
      "          {context}\n",
      "          \n",
      "          Question: {question}\n",
      "          Helpful Answer:\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834661f10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83466a880>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), graph_schema='Node properties are the following:\n",
      "          Chunk {id: STRING, source: STRING, text: STRING, page: INTEGER, textEmbedding: LIST},PDF {name: STRING},Microservice {name: STRING, technology: STRING},Dependency {name: STRING, type: STRING},Task {description: STRING, name: STRING, status: STRING, embedding: LIST},Team {name: STRING},Person {name: STRING}\n",
      "          Relationship properties are the following:\n",
      "          \n",
      "          The relationships are the following:\n",
      "          (:Chunk)-[:PART_OF]->(:PDF),(:Microservice)-[:DEPENDS_ON]->(:Microservice),(:Microservice)-[:DEPENDS_ON]->(:Dependency),(:Microservice)-[:MAINTAINED_BY]->(:Team),(:Task)-[:ASSIGNED_TO]->(:Team),(:Task)-[:LINKED_TO]->(:Microservice),(:Person)-[:MEMBER_OF]->(:Team),(:Person)-[:LEAD_OF]->(:Team)')>)], prompt=ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessage(content='You are a helpful AI assistant.'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]), output_parser=<class 'langchain.agents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser'>), tools=[Tool(name='Tasks', description='Useful when you need to answer questions about descriptions of tasks.\n",
      "                  Not useful for counting the number of tasks.\n",
      "                  Use full question as input.\n",
      "                  ', func=<bound method Chain.run of RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \n",
      "          If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "          ----------------\n",
      "          {context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834491190>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc834498fa0>, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Neo4jVector', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.neo4j_vector.Neo4jVector object at 0x7fc84df2c220>))>), Tool(name='Graph', description='Useful when you need to answer questions about microservices,\n",
      "                  their dependencies or assigned people. Also useful for any sort of \n",
      "                  aggregation like counting the number of tasks, etc.\n",
      "                  Use full question as input.\n",
      "                  ', func=<bound method Chain.run of GraphCypherQAChain(verbose=True, graph=<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7fc84ba4dac0>, cypher_generation_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'schema'], template='Task:Generate Cypher statement to query a graph database.\n",
      "          Instructions:\n",
      "          Use only the provided relationship types and properties in the schema.\n",
      "          Do not use any other relationship types or properties that are not provided.\n",
      "          Schema:\n",
      "          {schema}\n",
      "          Note: Do not include any explanations or apologies in your responses.\n",
      "          Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
      "          Do not include any text except the generated Cypher statement.\n",
      "          \n",
      "          The question is:\n",
      "          {question}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc8346536d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83465c460>, model_name='gpt-4', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), qa_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant that helps to form nice and human understandable answers.\n",
      "          The information part contains the provided information that you must use to construct an answer.\n",
      "          The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
      "          Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
      "          Here is an example:\n",
      "          \n",
      "          Question: Which managers own Neo4j stocks?\n",
      "          Context:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\n",
      "          Helpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\n",
      "          \n",
      "          Follow this example when generating answers.\n",
      "          If the provided information is empty, say that you don't know the answer.\n",
      "          Information:\n",
      "          {context}\n",
      "          \n",
      "          Question: {question}\n",
      "          Helpful Answer:\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834661f10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83466a880>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), graph_schema='Node properties are the following:\n",
      "          Chunk {id: STRING, source: STRING, text: STRING, page: INTEGER, textEmbedding: LIST},PDF {name: STRING},Microservice {name: STRING, technology: STRING},Dependency {name: STRING, type: STRING},Task {description: STRING, name: STRING, status: STRING, embedding: LIST},Team {name: STRING},Person {name: STRING}\n",
      "          Relationship properties are the following:\n",
      "          \n",
      "          The relationships are the following:\n",
      "          (:Chunk)-[:PART_OF]->(:PDF),(:Microservice)-[:DEPENDS_ON]->(:Microservice),(:Microservice)-[:DEPENDS_ON]->(:Dependency),(:Microservice)-[:MAINTAINED_BY]->(:Team),(:Task)-[:ASSIGNED_TO]->(:Team),(:Task)-[:LINKED_TO]->(:Microservice),(:Person)-[:MEMBER_OF]->(:Team),(:Person)-[:LEAD_OF]->(:Team)')>)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, tags=['openai-functions'], agent=OpenAIFunctionsAgent(llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc836eb01f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc836eb5b20>, model_name='gpt-4', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), tools=[Tool(name='Tasks', description='Useful when you need to answer questions about descriptions of tasks.\\n        Not useful for counting the number of tasks.\\n        Use full question as input.\\n        ', func=<bound method Chain.run of RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834491190>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc834498fa0>, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Neo4jVector', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.neo4j_vector.Neo4jVector object at 0x7fc84df2c220>))>), Tool(name='Graph', description='Useful when you need to answer questions about microservices,\\n        their dependencies or assigned people. Also useful for any sort of \\n        aggregation like counting the number of tasks, etc.\\n        Use full question as input.\\n        ', func=<bound method Chain.run of GraphCypherQAChain(verbose=True, graph=<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7fc84ba4dac0>, cypher_generation_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'schema'], template='Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc8346536d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83465c460>, model_name='gpt-4', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), qa_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nHere is an example:\\n\\nQuestion: Which managers own Neo4j stocks?\\nContext:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\\nHelpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\\n\\nFollow this example when generating answers.\\nIf the provided information is empty, say that you don't know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834661f10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83466a880>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), graph_schema='Node properties are the following:\\nChunk {id: STRING, source: STRING, text: STRING, page: INTEGER, textEmbedding: LIST},PDF {name: STRING},Microservice {name: STRING, technology: STRING},Dependency {name: STRING, type: STRING},Task {description: STRING, name: STRING, status: STRING, embedding: LIST},Team {name: STRING},Person {name: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Chunk)-[:PART_OF]->(:PDF),(:Microservice)-[:DEPENDS_ON]->(:Microservice),(:Microservice)-[:DEPENDS_ON]->(:Dependency),(:Microservice)-[:MAINTAINED_BY]->(:Team),(:Task)-[:ASSIGNED_TO]->(:Team),(:Task)-[:LINKED_TO]->(:Microservice),(:Person)-[:MEMBER_OF]->(:Team),(:Person)-[:LEAD_OF]->(:Team)')>)], prompt=ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessage(content='You are a helpful AI assistant.'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]), output_parser=<class 'langchain.agents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser'>), tools=[Tool(name='Tasks', description='Useful when you need to answer questions about descriptions of tasks.\\n        Not useful for counting the number of tasks.\\n        Use full question as input.\\n        ', func=<bound method Chain.run of RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834491190>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc834498fa0>, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Neo4jVector', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.neo4j_vector.Neo4jVector object at 0x7fc84df2c220>))>), Tool(name='Graph', description='Useful when you need to answer questions about microservices,\\n        their dependencies or assigned people. Also useful for any sort of \\n        aggregation like counting the number of tasks, etc.\\n        Use full question as input.\\n        ', func=<bound method Chain.run of GraphCypherQAChain(verbose=True, graph=<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7fc84ba4dac0>, cypher_generation_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'schema'], template='Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc8346536d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83465c460>, model_name='gpt-4', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), qa_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nHere is an example:\\n\\nQuestion: Which managers own Neo4j stocks?\\nContext:[manager:CTL LLC, manager:JANE STREET GROUP LLC]\\nHelpful Answer: CTL LLC, JANE STREET GROUP LLC owns Neo4j stocks.\\n\\nFollow this example when generating answers.\\nIf the provided information is empty, say that you don't know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc834661f10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc83466a880>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), graph_schema='Node properties are the following:\\nChunk {id: STRING, source: STRING, text: STRING, page: INTEGER, textEmbedding: LIST},PDF {name: STRING},Microservice {name: STRING, technology: STRING},Dependency {name: STRING, type: STRING},Task {description: STRING, name: STRING, status: STRING, embedding: LIST},Team {name: STRING},Person {name: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Chunk)-[:PART_OF]->(:PDF),(:Microservice)-[:DEPENDS_ON]->(:Microservice),(:Microservice)-[:DEPENDS_ON]->(:Dependency),(:Microservice)-[:MAINTAINED_BY]->(:Team),(:Task)-[:ASSIGNED_TO]->(:Team),(:Task)-[:LINKED_TO]->(:Microservice),(:Person)-[:MEMBER_OF]->(:Team),(:Person)-[:LEAD_OF]->(:Team)')>)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Tasks\",\n",
    "        func=vector_qa.run,\n",
    "        description=\"\"\"Useful when you need to answer questions about descriptions of tasks.\n",
    "        Not useful for counting the number of tasks.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Graph\",\n",
    "        func=cypher_chain.run,\n",
    "        description=\"\"\"Useful when you need to answer questions about microservices,\n",
    "        their dependencies or assigned people. Also useful for any sort of \n",
    "        aggregation like counting the number of tasks, etc.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "mrkl = initialize_agent(\n",
    "    tools, \n",
    "    ChatOpenAI(temperature=0, model_name='gpt-4'),\n",
    "    agent=AgentType.OPENAI_FUNCTIONS, verbose=True\n",
    ")\n",
    "ic(mrkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70838afb-4974-4e97-8569-c25f66518275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Graph` with `Which team is assigned to maintain PaymentService?`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Microservice {name: 'PaymentService'})-[:MAINTAINED_BY]->(t:Team) RETURN t.name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'t.name': 'TeamD'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mTeamD is assigned to maintain PaymentService.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| response: 'TeamD is assigned to maintain the PaymentService.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mTeamD is assigned to maintain the PaymentService.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TeamD is assigned to maintain the PaymentService.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = mrkl.run(\"Which team is assigned to maintain PaymentService?\")\n",
    "ic(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a832980-9c65-4826-8cf5-17fdad5b3cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Tasks` with `Which tasks have optimization in their description?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mTasks that have optimization in their description are:\n",
      "\n",
      "1. Optimize PaymentService\n",
      "2. Optimize AuthService\u001b[0m\u001b[32;1m\u001b[1;3mThe tasks that have optimization in their description are:\n",
      "\n",
      "1. Optimize PaymentService\n",
      "2. Optimize AuthService\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Which tasks have optimization in their description?', 'output': 'The tasks that have optimization in their description are:\\n\\n1. Optimize PaymentService\\n2. Optimize AuthService'}\n"
     ]
    }
   ],
   "source": [
    "response = mrkl.invoke(\"Which tasks have optimization in their description?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a812d19-ff7e-489c-9648-102f4eff52e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
